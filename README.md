# ğŸ­ Voice Emotion Recognition System

**Transform audio into emotional insights with AI-powered deep learning**

A production ready voice emotion recognition system that detects six distinct emotions from audio using advanced CNN-LSTM neural networks. Built with TensorFlow and trained on industry-standard RAVDESS and CREMA-D datasets.

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange)
![Flask](https://img.shields.io/badge/Flask-2.x-green)
![License](https://img.shields.io/badge/License-MIT-green)

---

## ğŸŒŸ What is Voice Emotion Recognition?

Voice Emotion Recognition leverages cutting-edge deep learning to analyze audio recordings and identify the speaker's emotional state. By combining **CNN** layers for feature extraction with **LSTM** networks for temporal pattern recognition, the system achieves robust emotion detection across diverse speaking styles and environments.

**Core Concept:**
- **Input:** Audio recording (3 seconds) or uploaded file
- **Process:** Feature extraction â†’ CNN-LSTM model â†’ Multi-head attention
- **Output:** Emotion prediction with confidence scores + transcription

---

## âœ¨ Key Features

### ğŸ¯ Intelligent Emotion Detection
Detects **six core emotions** with high accuracy: Neutral, Happy, Sad, Angry, Fear, and Surprise.

### ğŸŒ Dual Interface Options
- **Web UI:** Beautiful, responsive interface with real-time visualization
- **CLI Tool:** Command-line interface for quick testing and automation

### ğŸ¤ Flexible Audio Input
- Record directly from microphone (3-second clips)
- Upload audio files (WAV, MP3, M4A, FLAC, OGG)
- Built-in audio playback and history tracking

### ğŸ“ Speech Transcription
Automatic transcription using Google Speech Recognition API to see what was spoken alongside emotion detection.

### ğŸ“Š Comprehensive Analytics
- Confidence scores for all emotions
- Visual progress bars and emoji representations
- Recording history with timestamps

---

## ğŸ—ï¸ Architecture Overview

### Model Stack
| Component | Technology |
|-----------|-----------|
| Feature Extraction | MFCC (40 coefficients) + Spectral Features |
| Neural Network | CNN-LSTM with Multi-Head Attention |
| Optimizer | Adam (adaptive learning rate) |
| Regularization | Dropout + Batch Normalization + L2 |
| Augmentation | Noise, Time Shifting, Masking |

### Application Stack
| Layer | Technology |
|-------|-----------|
| Web Backend | Flask + CORS |
| Frontend | HTML5 + CSS3 + Vanilla JS |
| CLI | Python + SoundDevice |
| Audio Processing | Librosa + SoundFile |
| Model Format | Keras (.keras) |

---

## ğŸš€ Quick Start Guide

### Prerequisites
- Python 3.8+
- Microphone (for recording)
- Internet connection (for transcription)

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/yourusername/voice-emotion-recognition.git
cd voice-emotion-recognition

# 2. Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt
```

### Option 1: Web Interface

```bash
# Start the web server
python web_app.py

# Open browser and navigate to
http://localhost:5000
```

### Option 2: CLI Application

```bash
# Run the command-line interface
python app.py

# Commands:
# - Press Enter: Record 3 seconds
# - 'file path/to/audio.wav': Test audio file
# - 'samples': View sample sentences
# - 'list': Show recording history
# - 'quit': Exit
```

---

## ğŸ“ Project Structure

```
voice-emotion-recognition/
â”œâ”€â”€ ğŸ¯ Core Applications
â”‚   â”œâ”€â”€ app.py                    # CLI interface
â”‚   â”œâ”€â”€ web_app.py                # Flask web server
â”‚   â””â”€â”€ train.py                  # Model training script
â”‚
â”œâ”€â”€ ğŸ¨ Web Interface
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ index.html            # Main web UI
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ style.css             # Styling
â”‚       â”œâ”€â”€ script.js             # Frontend logic
â”‚       â””â”€â”€ uploads/              # User recordings
â”‚
â”œâ”€â”€ ğŸ§  Model & Data
â”‚   â”œâ”€â”€ models/                   # Trained models
â”‚   â”‚   â”œâ”€â”€ best_emotion_model.keras
â”‚   â”‚   â”œâ”€â”€ feature_scaler.pkl
â”‚   â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”‚   â””â”€â”€ training_history.png
â”‚   â””â”€â”€ data/                     # Organized datasets
â”‚       â”œâ”€â”€ angry/
â”‚       â”œâ”€â”€ happy/
â”‚       â”œâ”€â”€ sad/
â”‚       â”œâ”€â”€ fear/
â”‚       â”œâ”€â”€ neutral/
â”‚       â””â”€â”€ surprise/
â”‚
â”œâ”€â”€ âš™ï¸ Utilities
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ audio_preprocessing.py
â”‚       â””â”€â”€ dataset_loader.py
â”‚
â””â”€â”€ ğŸ”§ Configuration
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ .gitignore
    â””â”€â”€ README.md
```

---

## ğŸ“Š Dataset Setup

### Step 1: Download Datasets

**RAVDESS Dataset** (Ryerson Audio-Visual Database of Emotional Speech and Song)
- **Download Link:** [https://zenodo.org/records/1188976](https://zenodo.org/records/1188976)
- **Extract to:** `Audio_Speech_Actors_01-24/`
- **Size:** ~24 professional actors, 1,440 audio files
- **Format:** 16-bit WAV files at 48kHz

**CREMA-D Dataset** (Crowd-sourced Emotional Multimodal Actors Dataset)
- **Download Link:** [https://github.com/CheyneyComputerScience/CREMA-D](https://github.com/CheyneyComputerScience/CREMA-D)
- **Extract to:** `AudioWAV/`
- **Size:** 91 actors, 7,442 audio files
- **Format:** WAV files with varying sample rates

### Step 2: Organize Files

```bash
# Organize RAVDESS dataset into emotion folders
python organize_ravdess.py

# Organize CREMA-D dataset into emotion folders
python organize_crema_d.py
```

### Step 3: Verify Structure

Your `data/` folder should contain:
```
data/
â”œâ”€â”€ angry/       (~1,200 files)
â”œâ”€â”€ happy/       (~1,200 files)
â”œâ”€â”€ sad/         (~1,200 files)
â”œâ”€â”€ fear/        (~1,200 files)
â”œâ”€â”€ neutral/     (~1,200 files)
â””â”€â”€ surprise/    (~1,200 files)
```

---

## ğŸ‹ï¸ Training the Model

```bash
# Start training (100 epochs with early stopping)
python train.py
```

**Training Process:**
1. âœ… Load and organize ~7,000+ audio samples
2. âœ… Extract 120 audio features per sample
3. âœ… Apply data augmentation (2x factor)
4. âœ… Train CNN-LSTM with attention mechanism
5. âœ… Save best model based on validation accuracy
6. âœ… Generate confusion matrix and training plots

**Output Files:**
- `models/best_emotion_model.keras` - Best performing model
- `models/feature_scaler.pkl` - Feature normalization scaler
- `models/confusion_matrix.png` - Evaluation metrics
- `models/training_history.png` - Loss/accuracy curves

---

## ğŸ¯ Use Cases

### For Research & Education
- Study emotion detection algorithms
- Experiment with different architectures
- Analyze feature importance in emotion recognition

### For Product Development
- Voice assistants with emotion awareness
- Customer service sentiment analysis
- Mental health monitoring applications

### For Content Creators
- Analyze emotional tone in podcasts
- Evaluate voice-over performances
- Quality control for emotional delivery

---

## ğŸ”§ Technical Highlights

### Advanced Feature Extraction (120 features)
- **40 MFCC coefficients** - Voice characteristics
- **Spectral features** - Frequency domain analysis
- **Temporal features** - Energy and rhythm patterns
- **Chroma features** - Pitch class profiles

### Model Architecture
```
Input (174 timesteps, 120 features)
    â†“
CNN Blocks (64â†’128â†’256 filters)
    â†“
Bidirectional LSTM (128â†’64 units)
    â†“
Multi-Head Attention (4 heads)
    â†“
Dense Layers (256â†’128)
    â†“
Output (6 emotions, softmax)
```

### Training Optimizations
- **Class balancing** with computed weights
- **Learning rate scheduling** (reduce on plateau)
- **Data augmentation** (noise, shift, masking)
- **Early stopping** (patience: 15 epochs)

---

## ğŸŒ Web Interface Features

- **ğŸ¨ Aesthetic Design:** Cream and brown theme with smooth animations
- **ğŸ™ï¸ Direct Recording:** In-browser audio capture
- **ğŸ“ Drag & Drop:** Easy file uploads
- **ğŸ“ Transcription:** See what was spoken
- **ğŸ“Š Visual Analytics:** Progress bars and emoji feedback
- **ğŸ“œ History Tracking:** Review past recordings
- **ğŸ”„ Audio Replay:** Built-in player for saved recordings

---

## ğŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| Microphone not working | Check browser/system permissions |
| Model not found | Run `python train.py` first |
| Import errors | Reinstall: `pip install -r requirements.txt` |
| Transcription fails | Verify internet connection |
| Low accuracy | Ensure dataset is properly organized |

---

## ğŸ“ˆ Model Performance

- **Architecture:** CNN-LSTM with Multi-Head Attention
- **Training Samples:** 7,000+ augmented to 14,000+
- **Features per Sample:** 120 (MFCC + spectral + temporal)
- **Input Shape:** (174 timesteps, 120 features)
- **Output Classes:** 6 emotions
- **Optimizer:** Adam with adaptive learning rate
- **Regularization:** Dropout (0.3-0.5) + L2 + Batch Norm

---

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create a feature branch (`git checkout -b feature/Enhancement`)
3. ğŸ’¾ Commit changes (`git commit -m 'Add enhancement'`)
4. ğŸ“¤ Push to branch (`git push origin feature/Enhancement`)
5. ğŸ”„ Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ™ Acknowledgments

**Datasets:**
- **RAVDESS:** Livingstone SR, Russo FA (2018). The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS). [https://zenodo.org/records/1188976](https://zenodo.org/records/1188976)
- **CREMA-D:** Cao H, Cooper DG, Keutmann MK, Gur RC, Nenkova A, Verma R (2014). CREMA-D: Crowd-sourced Emotional Multimodal Actors Dataset. [https://github.com/CheyneyComputerScience/CREMA-D](https://github.com/CheyneyComputerScience/CREMA-D)

**Technologies:**
- TensorFlow/Keras - Deep learning framework
- Librosa - Audio feature extraction
- Flask - Web application framework

---

## ğŸ”® Roadmap

- [ ] Real-time streaming emotion detection
- [ ] Multi-language support
- [ ] Speaker identification
- [ ] Emotion intensity levels
- [ ] REST API for integration
- [ ] Docker containerization
- [ ] Mobile app (iOS/Android)

---

## ğŸ‘¨â€ğŸ’» Author

Built with â¤ï¸ for the AI and machine learning community

**â­ Star this repo if you find it helpful!**

---

**Voice Emotion Recognition - Where Audio Meets Emotional Intelligence**
